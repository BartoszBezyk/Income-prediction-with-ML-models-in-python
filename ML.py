# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AvZjs4zsx4xhag7D1hU0cln6PAPNmeja

# Loading and prepering the data
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import zscore
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv("adult.csv")
data.dropna()
data.head()

"""Converting columns to categorical or int."""

# List of columns to convert to categorical
cols_to_convert1 = ['workclass', 'education', 'marital-status', 'occupation',
                   'relationship', 'race', 'gender', 'native-country', 'income', 'educational-num']

# Convert specified columns to categorical type
data[cols_to_convert1] = data[cols_to_convert1].apply(lambda x: x.astype('category'))

# List of columns to convert to int
cols_to_convert2 = ['age', 'fnlwgt', 'capital-gain',
                   'capital-loss', 'hours-per-week']

# Convert specified columns to int type
data[cols_to_convert2] = data[cols_to_convert2].apply(lambda x: x.astype(int))

data.info()

# Create a dictionary to store the contingency tables
contingency_tables = {}

# Generate contingency tables for each categorical column
for col in cols_to_convert1:
    contingency_tables[col] = data[col].value_counts()

# Display the contingency tables
for col, table in contingency_tables.items():
    print(f"Contingency Table for {col}:\n")
    print(table)
    print("\n")

data.info()

"""Aggregations of values in columns"""

data['workclass'] = data['workclass'].replace({'Local-gov': 'SL-gov', 'Federal-gov': 'SL-gov'})
data['workclass'] = data['workclass'].replace({'Self-emp-inc': 'Self-emp', 'Self-emp-not-inc': 'Self-emp'})
data['workclass'].value_counts()

data['marital-status'] = data['marital-status'].replace({'Divorced' : 'Not-Married','Widowed': 'Not-Married'})
data['marital-status'] = data['marital-status'].replace({'Separated' : 'Married', 'Married-civ-spouse' : 'Married', 'Married-spouse-absent' : 'Married', 'Married-AF-spouse' : 'Married'})
data['marital-status'].value_counts()

# Replace specified values in the 'native.country' column
data['native-country'] = data['native-country'].replace({
    "United-States": "North-America", "Canada": "North-America", "Outlying-US(Guam-USVI-etc)": "North-America",
    "Mexico": "North-America", "Cuba": "North-America", "Jamaica": "North-America", "Haiti": "North-America",
    "Dominican-Republic": "North-America", "Guatemala": "North-America", "El-Salvador": "North-America",
    "Nicaragua": "North-America", "Trinadad&Tobago": "North-America", "Puerto-Rico": "North-America", "Honduras": "North-America",
    "England": "Europe", "Germany": "Europe", "Italy": "Europe", "Poland": "Europe", "Portugal": "Europe",
    "France": "Europe", "Yugoslavia": "Europe", "Scotland": "Europe", "Greece": "Europe",
    "Ireland": "Europe", "Hungary": "Europe", "Holand-Netherlands": "Europe",
    "Ecuador": "South-America", "Peru": "South-America", "Columbia": "South-America", "South": "South-America",
    "India": "Asia", "Iran": "Asia", "Philippines": "Asia", "Cambodia": "Asia",
    "Thailand": "Asia", "Laos": "Asia", "Taiwan": "Asia", "China": "Asia", "Japan": "Asia",
    "Vietnam": "Asia", "Hong": "Asia"
})

# Display unique values in the 'native-country' column
data['native-country'].value_counts()

"""Replacing "?" with NA"""

# Replace "?" with NaN
data.replace("?", pd.NA, inplace=True)

# Display the table of missing values
missing_values_table = data.isna().sum()
print(missing_values_table)

"""Removing NAs"""

data = data.dropna()

data.info()

"""Descriptive statistics of numerical values

"""

# Display basic statistics for numerical columns
numerical_summary = data.describe()
print(numerical_summary)

"""Histograms"""

# Select numerical columns
numerical_columns = data.select_dtypes(include=['number']).columns

# Set up the figure and axes
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))
axes = axes.flatten()

# Create histograms for each numerical column
for ax, col in zip(axes, numerical_columns):
    ax.hist(data[col].dropna(), bins=30, edgecolor='k', alpha=0.7)
    ax.set_title(f'Histogram of {col}')
    ax.set_xlabel(col)
    ax.set_ylabel('Frequency')

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()

"""Removing outliner"""

# Find the maximum value in the 'capital-gain' column
max_value = data['capital-gain'].max()
print(max_value)

# Remove rows with 'capital-gain' equal to 99999
data = data[data['capital-gain'] != 99999]

"""Splitting the data into traininf and testing set"""

data['income'] = data['income'].map({'<=50K': 0, '>50K': 1})

# Split the data into training and testing sets
train, test = train_test_split(data, test_size=0.2, random_state=123)
# Display the sizes of the training and testing sets
print(f"Training set size: {train.shape}")
print(f"Testing set size: {test.shape}")

X_train = train.drop("income", axis=1)
y_train = train["income"]
X_test = test.drop("income", axis=1)
y_test = test["income"]

# One-hot encoding
categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ],
    remainder='passthrough'
)

# Train and test data transformation
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

# Getting new names of features
encoded_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)
all_feature_names = np.concatenate([encoded_feature_names, X_train.drop(columns=categorical_cols).columns])


print(f"Number of features after one-hot encoding: {X_train_transformed.shape[1]}")
print(f"Number of features names: {len(all_feature_names)}")

"""## Creating models

Basic Logistic Regression model
"""

# Logistic Regression Model
logistic_model = LogisticRegression(random_state=123, max_iter= 10000)
logistic_model.fit(X_train_transformed, y_train)

# Predictions
logistic_predictions = logistic_model.predict(X_test_transformed)

# Evaluate the model
logistic_accuracy = accuracy_score(y_test, logistic_predictions)

print(f"Logistic Regression - Accuracy: {logistic_accuracy}")

"""Basic Random Forest model"""

# Random Forest Model
forest_model = RandomForestClassifier(random_state=123)
forest_model.fit(X_train_transformed, y_train)

# Predictions
forest_predictions = forest_model.predict(X_test_transformed)

# Evaluate the model
forest_accuracy = accuracy_score(y_test, forest_predictions)

print(f"Random Forest - Accuracy: {forest_accuracy}")

"""Basic Gradient Boosting model"""

# Gradient Boosting Model
boosting_model = GradientBoostingClassifier(random_state=123)
boosting_model.fit(X_train_transformed, y_train)

# Predictions
boosting_predictions = boosting_model.predict(X_test_transformed)

# Evaluate the model
boosting_accuracy = accuracy_score(y_test, boosting_predictions)

print(f"Boosting Trees - Accuracy: {boosting_accuracy}")

"""Using RFECV (Recursive Feature Elimination with Cross-Validation) to choose best features for our models. It recursivly eliminates features untill we get the best accuracy for our model. It is combined with cross-validaiton to get more statisticlly significant results."""

# Initialize the logistic regression model
logistic_model = LogisticRegression(random_state=123, max_iter=10000)

# Initialize RFECV with the logistic regression model
rfecv_logistic = RFECV(estimator=logistic_model, step=1, cv=StratifiedKFold(3), scoring='accuracy')

# Fit RFECV
rfecv_logistic.fit(X_train_transformed, y_train)

# Print the optimal number of features and the selected features
print(f"Optimal number of features for Logistic Regression: {rfecv_logistic.n_features_}")
selected_features_logistic = all_feature_names[rfecv_logistic.support_]
print(f"Selected features: {selected_features_logistic}")

# Train the logistic regression model with selected features
logistic_model.fit(X_train_transformed[:, rfecv_logistic.support_], y_train)

# Predictions
logistic_predictions = logistic_model.predict(X_test_transformed[:, rfecv_logistic.support_])

# Evaluate the model
logistic_accuracy = accuracy_score(y_test, logistic_predictions)
print(f"Optimized Logistic Regression - Accuracy: {logistic_accuracy}")

# Initialize the random forest model
forest_model = RandomForestClassifier(random_state=123)

# Initialize RFECV with the random forest model
rfecv_forest = RFECV(estimator=forest_model, step=1, cv=StratifiedKFold(5), scoring='accuracy')

# Fit RFECV
rfecv_forest.fit(X_train_transformed, y_train)

# Print the optimal number of features and the selected features
print(f"Optimal number of features for Random Forest: {rfecv_forest.n_features_}")
selected_features_forest = all_feature_names[rfecv_forest.support_]
print(f"Selected features: {selected_features_forest}")

# Train the random forest model with selected features
forest_model.fit(X_train_transformed[:, rfecv_forest.support_], y_train)

# Predictions
forest_predictions = forest_model.predict(X_test_transformed[:, rfecv_forest.support_])

# Evaluate the model
forest_accuracy = accuracy_score(y_test, forest_predictions)
print(f"Optimized Random Forest - Accuracy: {forest_accuracy}")

# Initialize the gradient boosting model
boosting_model = GradientBoostingClassifier(random_state=123)

# Initialize RFECV with the gradient boosting model
rfecv_boosting = RFECV(estimator=boosting_model, step=1, cv=StratifiedKFold(5), scoring='accuracy')

# Fit RFECV
rfecv_boosting.fit(X_train_transformed, y_train)

# Print the optimal number of features and the selected features
print(f"Optimal number of features for Gradient Boosting: {rfecv_boosting.n_features_}")
selected_features_boosting = all_feature_names[rfecv_boosting.support_]
print(f"Selected features: {selected_features_boosting}")

# Train the gradient boosting model with selected features
boosting_model.fit(X_train_transformed[:, rfecv_boosting.support_], y_train)

# Predictions
boosting_predictions = boosting_model.predict(X_test_transformed[:, rfecv_boosting.support_])

# Evaluate the model
boosting_accuracy = accuracy_score(y_test, boosting_predictions)
print(f"Optimized Gradient Boosting - Accuracy: {boosting_accuracy}")

"""## Evaluation

We evaluate models with metrics such as: accuracy, reccal, precision, sensitivity, specifity, AUC and ROC curve.
"""

def specificity_score(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    return tn / (tn + fp)

def evaluate_model(y_true, y_pred, y_prob, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    specificity = specificity_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)

    print(f"--- {model_name} ---")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"Specificity: {specificity}")
    print(f"F1 Score: {f1}")
    print(f"AUC: {roc_auc}")

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Receiver Operating Characteristic - {model_name}')
    plt.legend(loc="lower right")
    plt.show()

logistic_probabilities = logistic_model.predict_proba(X_test_transformed[:, rfecv_logistic.support_])[:, 1]
evaluate_model(y_test, logistic_predictions, logistic_probabilities, "Logistic Regression")

"""Accuracy = 0.83 means that 83% of ours predicions were real. It is good number but when we look at the other metrics such as reccal we can see result = 0.55. It means that we truly identified only 55% of real class 1 cases. We have hight result of accuracy because there are a lot more cases of class 0 in our data and our model usually predict it as class 0. AUC 0.88 is a good result, it means that our model have ability to determine wheather case belongs to class 0 or class 1. Using RFECV clearly improved this model from basic Logistic model."""

forest_probabilities = forest_model.predict_proba(X_test_transformed[:, rfecv_forest.support_])[:, 1]
evaluate_model(y_test, forest_predictions, forest_probabilities, "Random Forest")

"""Even though, RFECV didnt improve this model much, it is better that Logistic. Accuracy 0.84 isn't much higher but Recall is as 61%."""

boosting_probabilities = boosting_model.predict_proba(X_test_transformed[:, rfecv_boosting.support_])[:, 1]
evaluate_model(y_test, boosting_predictions, boosting_probabilities, "Gradient Boosting")

"""Accuracy is the highest in this model (0.86) but Recall (0.57) is worse that in Random Forest model. AUC is better so is ROC curve.

Choosing best model is hard here. It depends on what we really want to achive. If our main goal is to predict class 1 properly, then I would choose Random Forest model but if we want overall accuracy then Boosting Trees model will be better.
"""